{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cve_df = pd.read_csv(path_to_file)\n",
    "\n",
    "### Get only github activities\n",
    "github_df = cve_df.loc[cve_df['platform'] == 'github'].reset_index(drop=True)\n",
    "print(min(github_df['nodeTime']), max(github_df['nodeTime']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting repos by common CVEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_df1 = github_df[['nodeID', 'informationID']].copy()\n",
    "repos_df1.rename(columns={'nodeID': 'repoID1'}, inplace=True)\n",
    "repos_df1=repos_df1.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "repos_df2 = github_df[['nodeID', 'informationID']].copy()\n",
    "repos_df2.rename(columns={'nodeID': 'repoID2'}, inplace=True)\n",
    "repos_df2=repos_df2.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "### Merge both dataframes to link repos with common CVEs\n",
    "repos_df = pd.merge(repos_df1, repos_df2, on='informationID', how='left')\n",
    "\n",
    "### Get edge weight\n",
    "repos_df=repos_df.groupby(['repoID1','repoID2']).size().reset_index(name='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build graph for pairs of Repos connected by CVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nx = nx.from_pandas_edgelist(repos_df,'repoID1', 'repoID2', ['weight'], create_using=nx.Graph())\n",
    "print('Nodes:', g_nx.number_of_nodes(), 'Edges:', g_nx.number_of_edges())\n",
    "\n",
    "### Get largest strongly connected component\n",
    "lc_strong = sorted(nx.connected_components(g_nx), key=len, reverse=True)\n",
    "\n",
    "### Get Subgraph of the largest connected component.\n",
    "repos_lc = g_nx.subgraph(lc_strong[0])\n",
    "\n",
    "### Get list of nodes in largest connected component\n",
    "repos_nodes = list(repos_lc.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get daily activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dailyTime field\n",
    "github_df['dailyTime'] = github_df['nodeTime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#Get only repos in connected component\n",
    "github_acts=github_df.loc[github_df['nodeID'].isin(repos_nodes)].reset_index(drop=True)\n",
    "\n",
    "#Get daily activities of each repo for each of the 10 events\n",
    "github_dayacts_df = github_acts.groupby(['nodeID', 'dailyTime']).actionType.value_counts().unstack(fill_value=0).reset_index()\n",
    "\n",
    "### dailyTime to datetime object\n",
    "github_dayacts_df['dailyTime'] = pd.to_datetime(github_dayacts_df['dailyTime'])\n",
    "\n",
    "### Keep only PushEvents\n",
    "github_dayacts_df = github_dayacts_df[['nodeID', 'dailyTime', 'PushEvent']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate missing inactive days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date =  max(github_dayacts_df['dailyTime'])\n",
    "min_date = min(github_dayacts_df['dailyTime'])\n",
    "\n",
    "idx = pd.date_range(min_date, max_date)\n",
    "github_dayacts_df.set_index('dailyTime', inplace=True)\n",
    "\n",
    "df_concat = []\n",
    "\n",
    "for user, group in github_dayacts_df.groupby('nodeID'):\n",
    "    s = group['PushEvent']\n",
    "    \n",
    "    s = s.reindex(idx, fill_value=0)\n",
    "   \n",
    "    df = pd.DataFrame(s)\n",
    "    df['nodeID'] = user\n",
    "    \n",
    "    df_concat.append(df)\n",
    "    \n",
    "github_new_df = pd.concat(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_avg_daily = github_new_df.groupby('nodeID')['PushEvent'].mean().reset_index(name='avg_daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting to match DCRNN Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set dailyTime as a column in the df\n",
    "github_new_df.reset_index(level=0, inplace=True)\n",
    "github_new_df.rename(columns={'index':'nodeTime'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build output files for DCRNN\n",
    "\n",
    "#Turn largest connected component subgraph into a dataframe\n",
    "rt_lc_df = nx.to_pandas_edgelist(repos_lc)\n",
    "rt_lc_df['weight'] = 1\n",
    "#Find nodes with self-loops\n",
    "self_nodes = rt_lc_df.loc[rt_lc_df['source'] == rt_lc_df['target']]\n",
    "self_nodes = list(self_nodes['source'])\n",
    "\n",
    "nodes = list(set(repos_nodes) - set(self_nodes))\n",
    "\n",
    "#insert self-loops with weight 0 for records without self loops\n",
    "entries = [{'source':node, 'target':node, 'weight':0} for node in nodes]\n",
    "rt_lc_df= rt_lc_df.append(entries)\n",
    "#encode the largest connected component into integers\n",
    "rt_lc_encoding = {}\n",
    "for _i, node in enumerate(repos_nodes):\n",
    "    rt_lc_encoding[node] = str(_i)\n",
    "rt_lc_df['source'] = rt_lc_df['source'].apply(lambda x: rt_lc_encoding[x])\n",
    "rt_lc_df['target'] = rt_lc_df['target'].apply(lambda x: rt_lc_encoding[x])\n",
    "\n",
    "#rename columns\n",
    "rt_lc_df.rename(columns={'source':'from', 'target':'to', 'weight':'distance'}, inplace=True)\n",
    "rt_lc_df['distance']= rt_lc_df['distance'].astype(float)\n",
    "#write id list to file\n",
    "with open('', 'w') as f:\n",
    "    f.write(\",\".join(list(rt_lc_encoding.values())))\n",
    "rt_lc_df.to_csv('', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating timestep samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_new_df['label'] = github_new_df['nodeID'].map(rt_lc_encoding)\n",
    "rt_day_acts_formatted = github_new_df.pivot_table(values='PushEvent', index='nodeTime', columns='label', aggfunc='first')\n",
    "rt_day_acts_formatted.to_pickle('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vendor Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ven = ''\n",
    "mapping_df = pd.read_csv(path_to_ven)\n",
    "\n",
    "vendor_df = pd.read_csv(path_to_ven)\n",
    "vendor_df.rename(columns={'CVEID':'informationID'}, inplace=True)\n",
    "\n",
    "github_ven = pd.merge(github_df, vendor_df, on='informationID', how='inner')\n",
    "\n",
    "github_ven= github_ven.drop_duplicates(['actionType', 'informationID', 'nodeID', 'nodeTime',\n",
    "                                          'nodeUserID', 'Vendor']).reset_index(drop=True)\n",
    "\n",
    "vendor_df = github_ven[['informationID', 'Vendor']].copy()\n",
    "vendor_df = vendor_df.drop_duplicates()\n",
    "\n",
    "vendor_df1 = vendor_df.copy()\n",
    "vendor_df1.rename(columns={'Vendor':'Target'}, inplace=True)\n",
    "\n",
    "vendors_df = pd.merge(vendor_df, vendor_df1, on='informationID', how='left')\n",
    "\n",
    "vendors_df= vendors_df.groupby(['Vendor','Target']).size().reset_index(name='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create digraph for retweet diffusion\n",
    "g_nx_ven = nx.from_pandas_edgelist(vendors_df,'Vendor', 'Target', ['weight'], create_using=nx.Graph())\n",
    "print('Nodes:', g_nx_ven.number_of_nodes(), 'Edges:', g_nx_ven.number_of_edges())\n",
    "\n",
    "#Get largest strongly connected component\n",
    "lc_strong_ven = sorted(nx.connected_components(g_nx_ven), key=len, reverse=True)\n",
    "\n",
    "#Get Subgraph of the largest connected component.\n",
    "ven_lc = g_nx_ven.subgraph(lc_strong_ven[0])\n",
    "\n",
    "#Get list of nodes in largest connected component\n",
    "ven_nodes = list(ven_lc.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting activities by Vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_ven=github_ven.loc[github_ven['Vendor'].isin(ven_nodes)].reset_index(drop=True)\n",
    "\n",
    "### Get daily activities of each ven'' for 10 events\n",
    "github_dayacts_ven = github_ven.groupby(['Vendor', 'dailyTime']).actionType.value_counts().unstack(fill_value=0).reset_index()\n",
    "\n",
    "### dailyTime to datetime object\n",
    "github_dayacts_ven['dailyTime'] = pd.to_datetime(github_dayacts_ven['dailyTime'])\n",
    "\n",
    "### Keep only PushEvents\n",
    "github_dayacts_ven = github_dayacts_ven[['Vendor', 'dailyTime', 'PushEvent']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate missing inactive days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date =  max(github_dayacts_ven['dailyTime'])\n",
    "min_date = min(github_dayacts_ven['dailyTime'])\n",
    "\n",
    "idx = pd.date_range(min_date, max_date)\n",
    "github_dayacts_ven.set_index('dailyTime', inplace=True)\n",
    "\n",
    "df_concat = []\n",
    "\n",
    "for user, group in github_dayacts_ven.groupby('Vendor'):\n",
    "    s = group['PushEvent']\n",
    "    \n",
    "    s = s.reindex(idx, fill_value=0)\n",
    "   \n",
    "    df = pd.DataFrame(s)\n",
    "    df['Vendor'] = user\n",
    "    \n",
    "    df_concat.append(df)\n",
    "    \n",
    "github_new_ven = pd.concat(df_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting to match DCRNN Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set dailyTime as a column in the df\n",
    "github_new_ven.reset_index(level=0, inplace=True)\n",
    "github_new_ven.rename(columns={'index':'nodeTime'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build output files for DCRNN\n",
    "\n",
    "#Turn largest connected component subgraph into a dataframe\n",
    "rt_lc_df = nx.to_pandas_edgelist(ven_lc)\n",
    "rt_lc_df['weight'] = 1\n",
    "#Find nodes with self-loops\n",
    "self_nodes = rt_lc_df.loc[rt_lc_df['source'] == rt_lc_df['target']]\n",
    "self_nodes = list(self_nodes['source'])\n",
    "\n",
    "nodes = list(set(ven_nodes) - set(self_nodes))\n",
    "\n",
    "#insert self-loops with weight 0 for records without self loops\n",
    "entries = [{'source':node, 'target':node, 'weight':0} for node in nodes]\n",
    "rt_lc_df= rt_lc_df.append(entries)\n",
    "#encode the largest connected component into integers\n",
    "rt_lc_encoding = {}\n",
    "for _i, node in enumerate(ven_nodes):\n",
    "    rt_lc_encoding[node] = str(_i)\n",
    "rt_lc_df['source'] = rt_lc_df['source'].apply(lambda x: rt_lc_encoding[x])\n",
    "rt_lc_df['target'] = rt_lc_df['target'].apply(lambda x: rt_lc_encoding[x])\n",
    "\n",
    "#rename columns\n",
    "rt_lc_df.rename(columns={'source':'from', 'target':'to', 'weight':'distance'}, inplace=True)\n",
    "rt_lc_df['distance']= rt_lc_df['distance'].astype(float)\n",
    "#write id list to file\n",
    "with open('', 'w') as f:\n",
    "    f.write(\",\".join(list(rt_lc_encoding.values())))\n",
    "rt_lc_df.to_csv('', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating timestep samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_new_ven['label'] = github_new_ven['Vendor'].map(rt_lc_encoding)\n",
    "rt_day_acts_formatted = github_new_ven.pivot_table(values='PushEvent', index='nodeTime', columns='label', aggfunc='first')\n",
    "rt_day_acts_formatted.to_pickle('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_prod = ''\n",
    "mapping_df = pd.read_csv(path_to_prod)\n",
    "\n",
    "product_df = pd.read_csv(path_to_prod)\n",
    "product_df.rename(columns={'CVEID':'informationID'}, inplace=True)\n",
    "\n",
    "github_prod = pd.merge(github_df, product_df, on='informationID', how='inner')\n",
    "\n",
    "github_prod= github_prod.drop_duplicates(['actionType', 'informationID', 'nodeID', 'nodeTime',\n",
    "                                          'nodeUserID', 'Product']).reset_index(drop=True)\n",
    "\n",
    "product_df = github_prod[['informationID', 'Product']].copy()\n",
    "product_df = product_df.drop_duplicates()\n",
    "\n",
    "product_df1 = product_df.copy()\n",
    "product_df1.rename(columns={'Product':'Target'}, inplace=True)\n",
    "\n",
    "products_df = pd.merge(product_df, product_df1, on='informationID', how='left')\n",
    "\n",
    "products_df= products_df.groupby(['Product','Target']).size().reset_index(name='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create digraph for retweet diffusion\n",
    "g_nx_prod = nx.from_pandas_edgelist(products_df,'Product', 'Target', ['weight'], create_using=nx.Graph())\n",
    "print('Nodes: ', g_nx_prod.number_of_nodes(), 'Edges: ', g_nx_prod.number_of_edges())\n",
    "\n",
    "#Get largest strongly connected component\n",
    "lc_strong_prod = sorted(nx.connected_components(g_nx_prod), key=len, reverse=True)\n",
    "\n",
    "#Get Subgraph of the largest connected component.\n",
    "prod_lc = g_nx_prod.subgraph(lc_strong_prod[0])\n",
    "\n",
    "#Get list of nodes in largest connected component\n",
    "prod_nodes = list(prod_lc.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting activities by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_prod=github_prod.loc[github_prod['Product'].isin(prod_nodes)].reset_index(drop=True)\n",
    "\n",
    "#Get daily activities of each prod'' for 10 eprodts\n",
    "github_dayacts_prod = github_prod.groupby(['Product', 'dailyTime']).actionType.value_counts().unstack(fill_value=0).reset_index()\n",
    "\n",
    "#dailyTime to datetime object\n",
    "github_dayacts_prod['dailyTime'] = pd.to_datetime(github_dayacts_prod['dailyTime'])\n",
    "\n",
    "#Keep only PushEvents\n",
    "github_dayacts_prod = github_dayacts_prod[['Product', 'dailyTime', 'PushEvent']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate missing inactive days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date =  max(github_dayacts_prod['dailyTime'])\n",
    "min_date = min(github_dayacts_prod['dailyTime'])\n",
    "\n",
    "idx = pd.date_range(min_date, max_date)\n",
    "github_dayacts_prod.set_index('dailyTime', inplace=True)\n",
    "\n",
    "df_concat = []\n",
    "\n",
    "for user, group in github_dayacts_prod.groupby('Product'):\n",
    "    s = group['PushEvent']\n",
    "    \n",
    "    s = s.reindex(idx, fill_value=0)\n",
    "   \n",
    "    df = pd.DataFrame(s)\n",
    "    df['Product'] = user\n",
    "    \n",
    "    df_concat.append(df)\n",
    "    \n",
    "github_new_prod = pd.concat(df_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting to match DCRNN Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set dailyTime as a column in the df\n",
    "github_new_prod.reset_index(level=0, inplace=True)\n",
    "github_new_prod.rename(columns={'index':'nodeTime'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build output files for DCRNN\n",
    "\n",
    "#Turn largest connected component subgraph into a dataframe\n",
    "rt_lc_df = nx.to_pandas_edgelist(prod_lc)\n",
    "rt_lc_df['weight'] = 1\n",
    "#Find nodes with self-loops\n",
    "self_nodes = rt_lc_df.loc[rt_lc_df['source'] == rt_lc_df['target']]\n",
    "self_nodes = list(self_nodes['source'])\n",
    "\n",
    "nodes = list(set(prod_nodes) - set(self_nodes))\n",
    "\n",
    "#insert self-loops with weight 0 for records without self loops\n",
    "entries = [{'source':node, 'target':node, 'weight':0} for node in nodes]\n",
    "rt_lc_df= rt_lc_df.append(entries)\n",
    "#encode the largest connected component into integers\n",
    "rt_lc_encoding = {}\n",
    "for _i, node in enumerate(prod_nodes):\n",
    "    rt_lc_encoding[node] = str(_i)\n",
    "rt_lc_df['source'] = rt_lc_df['source'].apply(lambda x: rt_lc_encoding[x])\n",
    "rt_lc_df['target'] = rt_lc_df['target'].apply(lambda x: rt_lc_encoding[x])\n",
    "\n",
    "#rename columns\n",
    "rt_lc_df.rename(columns={'source':'from', 'target':'to', 'weight':'distance'}, inplace=True)\n",
    "rt_lc_df['distance']= rt_lc_df['distance'].astype(float)\n",
    "#write id list to file\n",
    "with open('/data/kinwaing_workspace/DCRNN/DCRNN/data/Github_Daily_Product/git_lc_nodes.txt', 'w') as f:\n",
    "    f.write(\",\".join(list(rt_lc_encoding.values())))\n",
    "rt_lc_df.to_csv('/data/kinwaing_workspace/DCRNN/DCRNN/data/Github_Daily_Product/git_lc_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating timestep samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_new_prod['label'] = github_new_prod['Product'].map(rt_lc_encoding)\n",
    "rt_day_acts_formatted = github_new_prod.pivot_table(values='PushEvent', index='nodeTime', columns='label', aggfunc='first')\n",
    "rt_day_acts_formatted.to_pickle('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pnnl_socialsim]",
   "language": "python",
   "name": "conda-env-pnnl_socialsim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
