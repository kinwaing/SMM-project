{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/conda/anaconda3/envs/pnnl_socialsim/lib/python3.6/site-packages/pysal/lib/weights/util.py:19: UserWarning: geopandas not available. Some functionality will be disabled.\n",
      "  warn('geopandas not available. Some functionality will be disabled.')\n",
      "/usr/local/conda/anaconda3/envs/pnnl_socialsim/lib/python3.6/site-packages/pysal/model/spvcm/abstracts.py:10: UserWarning: The `dill` module is required to use the sqlite backend fully.\n",
      "  from .sqlite import head_to_sql, start_sql\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import community \n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.algorithms.community import k_clique_communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to read json files for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(fn):\n",
    "\n",
    "    json_data = []\n",
    "    \n",
    "    if type(fn) == str:\n",
    "        with open(fn,'rb') as f:\n",
    "            for line in f:\n",
    "                json_data.append(json.loads(line))\n",
    "    else:\n",
    "        for fn0 in fn:\n",
    "            with open(fn0,'rb') as f:\n",
    "                for line in f:\n",
    "                    json_data.append(json.loads(line))\n",
    "\n",
    "    return(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to add missing days to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_data_git(df, entity='Vendor'):\n",
    "    \n",
    "    max_date =  max(df['dailyTime'])\n",
    "    min_date = min(df['dailyTime'])\n",
    "\n",
    "    idx = pd.date_range(min_date, max_date)\n",
    "    df.set_index('dailyTime', inplace=True)\n",
    "\n",
    "    df_concat = []\n",
    "\n",
    "    for user, group in df.groupby(entity):\n",
    "        s = group['PushEvent']\n",
    "\n",
    "        s = s.reindex(idx, fill_value=0)\n",
    "\n",
    "        df = pd.DataFrame(s)\n",
    "        df[entity] = user\n",
    "\n",
    "        df_concat.append(df)\n",
    "\n",
    "    df_new = pd.concat(df_concat)\n",
    "    return df_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate node lists and edge lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_files(G, nodes, file_nodes, file_edges):\n",
    "    \n",
    "    df = nx.to_pandas_edgelist(G)\n",
    "    df['weight'] = 1\n",
    "    #Find self loops\n",
    "    self_nodes = df.loc[df['source'] == df['target']]\n",
    "    self_nodes = list(self_nodes['source'])\n",
    "    \n",
    "    #Find nodes without self-loops\n",
    "    no_self_nodes = list(set(nodes) - set(self_nodes))\n",
    "    #insert self-loops with weight 0 for records without self loops\n",
    "    entries = [{'source':node, 'target':node, 'weight':0} for node in no_self_nodes]\n",
    "    \n",
    "    if len(entries) > 0:\n",
    "        df= df.append(entries)\n",
    "    \n",
    "    #encode the largest connected component into integers\n",
    "    lc_encoding = {}\n",
    "    for _i, node in enumerate(nodes):\n",
    "        lc_encoding[node] = str(_i)\n",
    "    df['source'] = df['source'].apply(lambda x: lc_encoding[x])\n",
    "    df['target'] = df['target'].apply(lambda x: lc_encoding[x])\n",
    "    \n",
    "    #rename columns\n",
    "    df.rename(columns={'source':'from', 'target':'to', 'weight':'distance'}, inplace=True)\n",
    "    df['distance']= df['distance'].astype(float)\n",
    "    \n",
    "    #write id list to file\n",
    "    with open(file_nodes, 'w') as f:\n",
    "        f.write(\",\".join(list(lc_encoding.values())))\n",
    "    df.to_csv(file_edges, index=False)\n",
    "    print('Files written succesfully')\n",
    "    \n",
    "    return df, lc_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = ''\n",
    "json_file = load_json(path_to_data)\n",
    "df_data = pd.DataFrame(json_file)\n",
    "\n",
    "path_wh = ''\n",
    "wh_df = pd.read_csv(path_wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use df_data to get correct parent mappings for retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rename column to match naming convention in white helmets dataset\n",
    "df_data.rename(columns={'tweet_id_h':'nodeID'}, inplace=True)\n",
    "\n",
    "#Twitter data\n",
    "wh_df = wh_df.loc[wh_df['platform'] == 'twitter'].reset_index(drop=True)\n",
    "\n",
    "#Drop duplicates\n",
    "wh_df = wh_df.drop_duplicates('nodeID', keep='last').reset_index(drop=True)\n",
    "\n",
    "#### Get only action types retweets\n",
    "wh_df_rt = wh_df.loc[wh_df['actionType'] == 'retweet'].reset_index(drop=True)\n",
    "\n",
    "### Get correct retweet chain\n",
    "wh_df_rt = pd.merge(wh_df_rt, df_data, on='nodeID')\n",
    "\n",
    "### Drop irrelevant information\n",
    "wh_df_rt = wh_df_rt.drop(columns=['parentID', 'rootID','tweet_postdate', 'tweeter_UTC_Offset', 'tweeter_followers'])\n",
    "wh_df_rt.rename(columns={'retweeted_from_tweet_id_h':'parentID', 'source_tweet_id_h':'rootID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get correct parentUserID for parentID field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ParentUserID -> ParentID mapping\n",
    "wh_mapping = wh_df_new[['nodeID', 'nodeUserID']]\n",
    "wh_mapping.rename(columns={'nodeID':'parentID', 'nodeUserID':'parentUserID'}, inplace=True)\n",
    "\n",
    "#### Include parent user ID for the retweet dataframe\n",
    "wh_df_rt = pd.merge(wh_mapping, wh_df_rt, on='parentID')\n",
    "wh_df_rt = wh_df_rt[['nodeID', 'nodeUserID', 'parentID', 'parentUserID', 'nodeTime']]\n",
    "\n",
    "### nodeTime to datetime object\n",
    "wh_df_rt['nodeTime'] = pd.to_datetime(wh_df_rt['nodeTime'])\n",
    "\n",
    "#Build retweet network\n",
    "rt_graph = wh_df_rt.groupby(['nodeUserID', 'parentUserID']).size().reset_index(name='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WH-Daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build directed graph for Retweet Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Create digraph for retweet diffusion\n",
    "g_nx = nx.from_pandas_edgelist(rt_graph,'parentUserID', 'nodeUserID', ['weight'], create_using=nx.DiGraph())\n",
    "print('Nodes:',g_nx.number_of_nodes(), 'Edges:', g_nx.number_of_edges())\n",
    "\n",
    "###Get lists of largest strongly connected component\n",
    "lc_strong = sorted(nx.strongly_connected_components(g_nx), key=len, reverse=True)\n",
    "\n",
    "###Get Subgraph of the largest connected component.\n",
    "rt_lc = g_nx.subgraph(lc_strong[0])\n",
    "\n",
    "#Number of nodes and edges in largest connected component\n",
    "print('Nodes:', rt_lc.number_of_nodes(), 'Edges:', rt_lc.number_of_edges())\n",
    "\n",
    "###Get list of nodes in largest connected component\n",
    "rt_nodes = list(rt_lc.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get daily number of activities, and fill inactive days with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keep only activities for nodes in largest connected component\n",
    "rt_df = wh_df_rt.loc[wh_df_rt['nodeUserID'].isin(rt_nodes)].reset_index(drop=True)\n",
    "\n",
    "###Create a dailyTime field\n",
    "rt_df['dailyTime'] = rt_df['nodeTime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "###Compute daily activities of each node\n",
    "rt_df_daily_acts = rt_df.groupby(['nodeUserID','dailyTime']).size().reset_index(name='daily_acts')\n",
    "\n",
    "###dailyTime to datetime object\n",
    "rt_df_daily_acts['dailyTime'] = pd.to_datetime(rt_df_daily_acts['dailyTime'])\n",
    "\n",
    "###Generate missing data\n",
    "rt_new_df = add_missing_data_git(rt_df_daily_acts, entity='nodeUserID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute monthly activities for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set dailyTime as a column in the df\n",
    "rt_new_df.reset_index(level=0, inplace=True)\n",
    "rt_new_df.rename(columns={'index':'nodeTime'}, inplace=True)\n",
    "\n",
    "#Create new monthly time column\n",
    "rt_new_df['monthTime'] = rt_new_df['nodeTime'].dt.strftime('%Y-%m')\n",
    "\n",
    "#Compute the number of monthly activities for each node\n",
    "rt_month_acts = rt_new_df.groupby(['nodeUserID', 'monthTime'])['daily_acts'].sum().reset_index(name='monthly_acts')\n",
    "\n",
    "### Avg. monhtly number of activities\n",
    "rt_avg_monthly_acts = rt_month_acts.groupby('nodeUserID')['monthly_acts'].mean().reset_index(name='avg_acts')\n",
    "print(rt_avg_monthly_acts.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute avg. number of daily activities per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_avg_daily_acts = rt_new_df.groupby('nodeUserID')['daily_acts'].mean().reset_index(name='avg_acts')\n",
    "print(rt_avg_daily_acts.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating timestep samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_path = ''\n",
    "nodelist_path = ''\n",
    "edgelist, encoding = generate_input_files(rt_new_df, rt_nodes, nodelist_path, edgelist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ''\n",
    "rt_day_acts_formatted = rt_new_df.pivot_table(values='daily_acts', index='nodeTime', columns='label', aggfunc='first')\n",
    "rt_day_acts_formatted.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WH-Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform digraph to undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_lc_un = rt_lc.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6376"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_lc_un.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Louvain algorithm in Connected graph to get communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute the best partition using Louvain\n",
    "partition = community.best_partition(rt_lc_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_un = {}\n",
    "for user, label in partition.items():\n",
    "    community_un[label] = community_un.get(label, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 961,\n",
       " 1: 2296,\n",
       " 2: 1345,\n",
       " 3: 787,\n",
       " 4: 165,\n",
       " 5: 459,\n",
       " 6: 106,\n",
       " 7: 38,\n",
       " 8: 193,\n",
       " 9: 7,\n",
       " 10: 4,\n",
       " 11: 9,\n",
       " 12: 2,\n",
       " 13: 2,\n",
       " 14: 2}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_un"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign Users to corresponding clusters found with Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_df_rt['label'] = wh_df_rt['nodeUserID'].map(partition)\n",
    "\n",
    "#Find only activities within strongly connected component\n",
    "rt_cluster = wh_df_rt.loc[(wh_df_rt['nodeUserID'].isin(rt_nodes)) &\n",
    "                          (wh_df_rt['parentUserID'].isin(rt_nodes))].reset_index(drop=True)\n",
    "\n",
    "### Map users to correct grouping\n",
    "rt_cluster['label_source'] = rt_cluster['nodeUserID'].map(partition)\n",
    "rt_cluster['label_parent'] = rt_cluster['parentUserID'].map(partition)\n",
    "\n",
    "### Edgelist\n",
    "rt_cluster_graph = rt_cluster.groupby(['label_source', 'label_parent']).size().reset_index(name='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build directed graph for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create digraph for retweet diffusion\n",
    "g_nx_cluster = nx.from_pandas_edgelist(rt_cluster_graph,'label_parent', 'label_source', ['weight'], create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get list of nodes in largest connected component\n",
    "rt_nodes_cluster = list(g_nx_cluster.nodes())\n",
    "print('Nodes:', g_nx_cluster.number_of_nodes(), 'Edges: ', g_nx_cluster.number_of_edges())\n",
    "\n",
    "### Keep only activities for nodes in largest connected component\n",
    "rt_df_cluster = wh_df_rt.loc[wh_df_rt['label'].isin(rt_nodes_cluster)].reset_index(drop=True)\n",
    "\n",
    "rt_df_cluster['label'] = rt_df_cluster['label'].astype(int)\n",
    "\n",
    "### Create a dailyTime field\n",
    "rt_df_cluster['dailyTime'] = rt_df_cluster['nodeTime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "### Compute daily activities of each node\n",
    "rt_df_cluster_acts = rt_df_cluster.groupby(['label','dailyTime']).size().reset_index(name='daily_acts')\n",
    "\n",
    "### dailyTime to datetime object\n",
    "rt_df_cluster_acts['dailyTime'] = pd.to_datetime(rt_df_cluster_acts['dailyTime'])\n",
    "\n",
    "### Get average number of activities per cluster\n",
    "rt_avg_daily_acts_cluster = rt_df_cluster_acts.groupby('label')['daily_acts'].mean().reset_index(name='avg_acts')\n",
    "\n",
    "### Generate missing data\n",
    "rt_new_df_cluster = add_missing_data_git(rt_df_cluster_acts, entity='label')\n",
    "\n",
    "\n",
    "### Set dailyTime as a column in the df\n",
    "rt_new_df_cluster.reset_index(level=0, inplace=True)\n",
    "rt_new_df_cluster.rename(columns={'index':'nodeTime'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating timestep samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_path = ''\n",
    "nodelist_path = ''\n",
    "edgelist, encoding = generate_input_files(rt_new_df_cluster, rt_nodes_cluster, nodelist_path, edgelist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ''\n",
    "rt_day_acts_formatted = rt_new_df_cluster.pivot_table(values='daily_acts', index='nodeTime', columns='label', aggfunc='first')\n",
    "rt_day_acts_formatted.to_pickle(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pnnl_socialsim]",
   "language": "python",
   "name": "conda-env-pnnl_socialsim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
